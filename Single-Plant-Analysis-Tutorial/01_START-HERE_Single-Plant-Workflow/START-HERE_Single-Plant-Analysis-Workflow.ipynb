{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finnish-showcase",
   "metadata": {
    "id": "finnish-showcase"
   },
   "source": [
    "# <h1 style=\"font-size:4rem;color:orange;\"><i>Arabidopsis thaliana</i> -  Single Plant Workflow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-compound",
   "metadata": {
    "id": "million-compound",
    "tags": []
   },
   "source": [
    "# Loading Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-airline",
   "metadata": {
    "id": "shared-airline",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Matplotlib enables us to plot within the notebook, matplotlib is very powerful plotting library\n",
    "%matplotlib widget\n",
    "# Imports NumPy package into notebook, essential for scientific computing\n",
    "import numpy as np\n",
    "# Imports PlantCV into notebook so that we can conduct plant phenotyping analyses\n",
    "from plantcv import plantcv as pcv\n",
    "# Imports library to handle workflow inputs compatible with parallel workflow execution.\n",
    "from plantcv.parallel import WorkflowInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-brass",
   "metadata": {
    "id": "latin-brass",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input/output options\n",
    "args = WorkflowInputs(\n",
    "    images=[\"\"],\n",
    "    names=\"\",\n",
    "    result=\"\",\n",
    "    outdir=\".\",\n",
    "    writeimg=True,\n",
    "    debug=\"plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-hawaii",
   "metadata": {
    "id": "increasing-hawaii"
   },
   "source": [
    "# Setting Your PlantCV Environment Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-maple",
   "metadata": {
    "id": "senior-maple"
   },
   "source": [
    "**For more information on the class Params, check out**\n",
    "https://plantcv.readthedocs.io/en/4.x/params/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-school",
   "metadata": {
    "id": "improving-school",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set debug to the global parameter\n",
    "pcv.params.debug = args.debug\n",
    "# Change display settings\n",
    "pcv.params.dpi = 100\n",
    "# Now we set up our parameters for our PlantCV environment\n",
    "pcv.params.text_size = 20\n",
    "pcv.params.text_thickness = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-version",
   "metadata": {
    "id": "infinite-version"
   },
   "source": [
    "Now, let's start working with some color images. We are going to begin by reading in a sample *A. thaliana* image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-likelihood",
   "metadata": {
    "id": "quantitative-likelihood"
   },
   "source": [
    "# Reading Images into PlantCV\n",
    "Inputs:\n",
    "* filename = name of image file\n",
    "* mode     = mode of imread (\"native\", \"rgb\", \"rgba\", \"gray\", \"csv\", \"envi\", \"arcgis\"). **Default is \"native.\"**\n",
    "\n",
    "Returns:\n",
    "* img      = image object as numpy array\n",
    "* path     = path to image file\n",
    "* img_name = name of image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-booking",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "cf9c420372ae425b98e4e113c592d7cd",
      "e1e4dd89b16445728f2662ad36617814"
     ]
    },
    "id": "anonymous-booking",
    "outputId": "dfc885a4-f8b8-49d1-d24f-13751d46ede6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the sample image of Arabidopsis into PlantCV.\n",
    "# Leave **mode** as Default - (\"native).\n",
    "img, path, img_name = pcv.readimage(filename=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-johnston",
   "metadata": {
    "id": "abstract-johnston"
   },
   "source": [
    "# Investigating the Dimensions of Your Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-puppy",
   "metadata": {
    "id": "average-puppy",
    "outputId": "6326a0dc-532f-46e5-c6f0-f06b4e001290",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the shape and size of our RGB image below:\n",
    "#The output will tell us (# of rows, # of columns, # of color channels)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-medication",
   "metadata": {
    "id": "impaired-medication",
    "outputId": "d440942f-ce43-4f84-f5bb-e79967280229",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Determine the data type of the image below\n",
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-concord",
   "metadata": {
    "id": "fifteen-concord",
    "outputId": "797a0bbb-520a-41eb-c307-94f105fb10d4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Identify the minimum pixel value found in the image between all three channels\n",
    "np.min(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-queensland",
   "metadata": {
    "id": "solid-queensland",
    "outputId": "a43b8bef-460d-4597-ae36-12a4672fc8bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Identify the maximum pixel value found in our image between all three channels\n",
    "np.max(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-rebound",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f2011711cd684694b0a50e08cc6889d0",
      "37cd27828cff4b918fa561bd8d71b203"
     ]
    },
    "id": "blank-rebound",
    "outputId": "cf765f29-51fb-42db-bd67-4c56b262cc11",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We described earlier that each color channel is its own matrix\n",
    "#To show this, we will pull out the green channel and it will show each is a grayscale channel with its own respective data\n",
    "#We will use pcv.plot_image(img=img[:,:,1]) to extract the green channel data\n",
    "#[:,:,0] -> Blue channel\n",
    "#[:,:,1] -> Green channel\n",
    "#[:,:,2] -> Red channel\n",
    "pcv.plot_image(img=img[:, :, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-transfer",
   "metadata": {
    "id": "hydraulic-transfer"
   },
   "source": [
    "**What is happening in the other channels?**\n",
    "* Change the 3rd element to 0 or 2 to see what the Blue and Red channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-milan",
   "metadata": {
    "id": "abandoned-milan",
    "outputId": "ed476525-8a4c-447c-81c0-4120ba8b943c",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate the min, max, and mean values from the green channel to see how many pixels are represented in that channel\n",
    "#We can check out the \"pixel stats\" for each channel to see where the most pixel intensity exists.\n",
    "print(np.min(img[:,:,0]))\n",
    "print(np.max(img[:,:,0]))\n",
    "print(np.mean(img[:,:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-november",
   "metadata": {
    "id": "rolled-november"
   },
   "source": [
    "# Investigating Colorspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-contamination",
   "metadata": {
    "id": "tracked-contamination"
   },
   "source": [
    "For image analysis and visual perception of color properties, other color models such as [Hue, Saturation, and Value (HSV)](https://en.wikipedia.org/wiki/HSL_and_HSV) or [CIELAB (LAB)](https://en.wikipedia.org/wiki/CIELAB_color_space) have advantages over RGB.\n",
    "\n",
    "In the next exercise we are going to visualize the color spaces available in PlantCV so we can label the plant material and distinguish the plant from the background.\n",
    "We will use the function *pcv.visualize.colorspaces()* to help us see our image (img) in the various color models (HSV and LAB). We need to store the color space plot in a variable so just name the new variable *cs*. The last thing we will do is set original_img to **False** as we don't need to see our original image, we only care about the color space.\n",
    "\n",
    "As you start typing out the method, be sure to use the TAB key to autocomplete the method so you don't end up with typographical errors. Once you complete typing *pcv.visualize.colorspaces()*, press SHIFT + TAB to view the helper to see how to set up the method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-deposit",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "91ca177dd0f044e5b24285f6362014d9",
      "9942e163a8f0404882dc8fc3186c72bb"
     ]
    },
    "id": "clear-deposit",
    "outputId": "ea72de01-f83c-44fd-95db-72cc509db601",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize component HSV and LAB color spaces\n",
    "cs = pcv.visualize.colorspaces(rgb_img=, \n",
    "                               original_img=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-lender",
   "metadata": {
    "id": "framed-lender"
   },
   "source": [
    "Let's take a moment to understand what each of the color models above are showing us. We will start with describing what HSV color spaces can tell us:\n",
    "<p style=\"text-align:center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Hsl-hsv_models.svg/240px-Hsl-hsv_models.svg.png\" />\n",
    "\n",
    "* Hue - refers to the color of the pixel, the absolute representation of the color\n",
    "* Saturation - refers to how \"colorful\" the pixel is (i.e., the difference between light green and forest green)\n",
    "* Value - refers to how \"white\" the pixel is (0-255)\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Visible_gamut_within_CIELAB_color_space_D65_whitepoint_mesh.png/240px-Visible_gamut_within_CIELAB_color_space_D65_whitepoint_mesh.png\" />\n",
    "\n",
    "* Lightness - similar to the Value color space, how much \"white\" or \"brightness\" is in the color\n",
    "* A - This color space describes the green/red-magenta values\n",
    "* B - This color spaces describes the blue/yellow values\n",
    "\n",
    "We are visualizing our plant material in these spaces to maximize the differences between the \"plant material\" and the \"background\".\n",
    "In our image, these differences will be more easily observed. When we use use top view images, looking at our images in these color spaces are very important as they will help us discern \"plant material\" from \"soil media\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-breach",
   "metadata": {
    "id": "grave-breach"
   },
   "source": [
    "Use this color space that you determined to have the greatest contrast between plant and background, we are now going to convert our RGB image and cast it into grayscale but in the channel you chose.\n",
    "\n",
    "(If you are unsure how to set up the method, press SHIFT + TAB to access the helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-advisory",
   "metadata": {
    "id": "fabulous-advisory"
   },
   "source": [
    "# Convert Image to Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-attraction",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ca0c9d35ccab47f096995c2178bc2aa1",
      "83291ccbf9db447db3abbead3b849ce0"
     ]
    },
    "id": "becoming-attraction",
    "outputId": "a9613b4b-5813-4d31-8088-80f88aa8bf70",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The output of this method will be stored in the variable named *gray_img* since we are\n",
    "# producing a grayscale image.\n",
    "# The functions we will be using are *pcv.rgb2gray_hsv()* or *pcv.rgb2gray_lab()*\n",
    "# Set the rgb_img to our *img* and define the channel with the color space you thought had\n",
    "# the greatest contrast.\n",
    "\n",
    "# Convert the RGB image into a grayscale image by choosing one of the HSV or LAB channels\n",
    "gray_img = pcv.rgb2gray_lab(rgb_img=, \n",
    "                            channel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-grant",
   "metadata": {
    "id": "rising-grant"
   },
   "source": [
    "Now that we have our grayscale image, let's see which pixels refer to our plant and which are the background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-baseline",
   "metadata": {
    "id": "ignored-baseline"
   },
   "source": [
    "## Visualizing pixel distribution in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-elevation",
   "metadata": {
    "id": "unexpected-elevation"
   },
   "source": [
    "Inputs:\n",
    "* img            = an RGB or grayscale image to analyze\n",
    "* mask           = binary mask, calculate histogram from masked area only (default=None)\n",
    "* bins           = divide the data into n evenly spaced bins (default=100)\n",
    "* lower_bound    = the lower bound of the bins (x-axis min value) (default=None)\n",
    "* upper_bound    = the upper bound of the bins (x-axis max value) (default=None)\n",
    "* title          = a custom title for the plot (default=None)\n",
    "* hist_data      = return the frequency distribution data if True (default=False)\n",
    "\n",
    "Returns:\n",
    "* fig_hist       = histogram figure\n",
    "* hist_df        = dataframe with histogram data, with columns \"pixel intensity\" and \"proportion                    of pixels (%)\"\n",
    "*Look at your picture, what percentage of the picture is plant versus background? It will be helpful to keep this in mind when you look at the histogram output.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-hospital",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f275327a19f84f568292b11faf3b99cd",
      "fdc8a053e992460daf7420250ab1ef23"
     ]
    },
    "id": "configured-hospital",
    "outputId": "75d1c079-caef-439b-d89f-334a26ee2b1b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use *pcv.visualize.histogram()* to see the distribution of pixel values in the grayscale\n",
    "# and store it in the variable *hist*.\n",
    "# We will create **50 bins** to start with but play with the bins until you can see\n",
    "# discrete peaks.\n",
    "# Set *img* to *gray_img* since that is the image whose pixel distribution we wish to see.\n",
    "\n",
    "# Visualize a histogram of the grayscale values to identify signal related to the plant\n",
    "# vs the background.\n",
    "pcv.plot_image(gray_img)\n",
    "hist = pcv.visualize.histogram(img=, \n",
    "                               bins=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-secretary",
   "metadata": {
    "id": "sitting-secretary"
   },
   "source": [
    "What does your chart look like? Compare your histogram with your neighbor's and see where the peaks are on the chart. As a refresher, histograms aggregate information into discrete bins that satisfy a range of values. The more values that fall within that \"bin\", the larger the peak on the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-trademark",
   "metadata": {
    "id": "seeing-trademark"
   },
   "outputs": [],
   "source": [
    "print(input(\"Where is the largest peak located on the histogram?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-netscape",
   "metadata": {
    "id": "ahead-netscape"
   },
   "outputs": [],
   "source": [
    "print(input(\"What part of our picture do you think is the big peak, is it the plant or the background?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-gates",
   "metadata": {
    "id": "looking-gates"
   },
   "source": [
    "*Discuss amongst yourselves what the other peaks on the histogram are referring to in our image and change the number of bins in the histogram to see how the pixel intensities are sorted. How did your finds compare with what you just found out?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-tribe",
   "metadata": {
    "id": "seventh-tribe"
   },
   "source": [
    "# Creating a Mask - Binary Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-theorem",
   "metadata": {
    "id": "combined-theorem"
   },
   "source": [
    "The next step is to create a binary mask that excludes pixel data from the background, but shows the pixel intensities from the plant material. To do this we will use the function *pcv.threshold.binary()* to set a binary threshold that labels the plant pixels white and the background as black.\n",
    "\n",
    "Inputs:\n",
    "* gray_img     = Grayscale image data\n",
    "* threshold    = Threshold value (0-255)\n",
    "* object_type  = \"light\" or \"dark\" (default: \"light\")\n",
    "               - If object is lighter than the background then standard thresholding is done\n",
    "               - If object is darker than the background then inverse thresholding is done\n",
    "\n",
    "Returns:\n",
    "* bin_img      = Thresholded, binary image\n",
    "\n",
    "(Press SHIFT + TAB key to see the helper so we can set the inputs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-ballet",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "29b57627b5fd48688c929b1a8c6d2e0e",
      "94eb93c440b448008677d9759e583a67"
     ]
    },
    "id": "underlying-ballet",
    "outputId": "0f3e01a8-51df-472a-ae37-4351f20f8d26",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are going to store the information from this binary threshold in the variable bin_img.\n",
    "\n",
    "# Use the histogram to set a binary threshold where the plant pixels will be labeled white\n",
    "# and the background will be labeled black\n",
    "man_bin_img = pcv.threshold.binary(gray_img=,\n",
    "                                   threshold=,\n",
    "                                   object_type=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-mixture",
   "metadata": {
    "id": "passing-mixture"
   },
   "source": [
    "Congratulations! You just set a manual threshold to exclude pixel data that you found does not represent the plant material.\n",
    "\n",
    "PlantCV has the ability to automatically threshold using the function *pcv.threshold.otsu()* - our first step into using machine learning approaches.\n",
    "\n",
    "(Use SHIFT + TAB to observe the helper and see how to set up this method, it isn't too much different than setting up a manual binary threshold.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-orbit",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "8a771b03234a4e7180ba3eba3bda2d0c",
      "bbb95963fcae4c0ba743c684537a6c34"
     ]
    },
    "id": "alpine-orbit",
    "outputId": "39ec1bab-4f5d-43cc-bb59-9ab0be647256",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instead of setting a manual threshold, try an automatic threshold method such as Otsu\n",
    "auto_bin_img = pcv.threshold.otsu(gray_img=,\n",
    "                                  object_type=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-witch",
   "metadata": {
    "id": "analyzed-witch"
   },
   "source": [
    "You might notice that some of the pixels from the background and color card made it into our Otsu-thresholded image, disregard these as they will not affect our analyses due on the design of our PlantCV workflow. What has happened is that the algorithm that Otsu's thresholding technique determined a pixel range that excluded the majority of the background, pot, and color card except for a few pixels (due to shading/color gradients).\n",
    "\n",
    "Otsu's thresholding technique runs an algorithm with the following steps:\n",
    "1. Process the input image\n",
    "2. Obtain image histogram (distribution of pixels)\n",
    "3. Compute the threshold value T\n",
    "4. Replace image pixels into white in those regions, where saturation is greater than T and into the black in the opposite cases.\n",
    "\n",
    "For more information on Otsu's thresholing technique, visit https://learnopencv.com/otsu-thresholding-with-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-proportion",
   "metadata": {
    "id": "affiliated-proportion"
   },
   "source": [
    "We need to look at our manually-thresholded and the Otsu-generated binary masks to determine which of these binary masks contain the most \"plant.\" This will be the binary mask that we use to further filter our mask for overlaying on our image for precise measurement.\n",
    "\n",
    "In the previous step, we created an ROI circle that encapsulated as much as of the plant as possible. Since we care about total plant area, we want the binary mask that will contain the most plant material (i.e., the Otsu-generated binary mask - *auto_bin_img*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-girlfriend",
   "metadata": {
    "id": "blessed-girlfriend"
   },
   "source": [
    "## Gathering Object Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-convert",
   "metadata": {
    "id": "committed-convert"
   },
   "source": [
    "The function *pcv.visualize.obj_sizes()* will display all of the objects in the image using your binary mask and original image. Investigate the properties of this function and play around with the **num_objects** parameter to determine how many objects are in the image and the sizes of those objects.\n",
    "Inputs:\n",
    "* img          = RGB or grayscale image data\n",
    "* mask         = Binary mask made from filtered binary mask\n",
    "* num_objects  = Optional parameter to limit the number of objects that will get annotated\n",
    "\n",
    "Returns:\n",
    "* plotting_img = Plotting image with objects labeled by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-antibody",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d943d2fb3c284fe5989ea2235fb3d2ec",
      "172df376acea4cd5a4baa323373ecac3"
     ]
    },
    "id": "mathematical-antibody",
    "outputId": "ecc843e5-b9f0-4871-9301-df094e308cf7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust the plot parameters so we can read the numbers.\n",
    "pcv.params.text_size = 3\n",
    "pcv.params.text_thickness = 5\n",
    "\n",
    "# Set the number of objects to 10 so we can see the size of the largest objects\n",
    "sizes = pcv.visualize.obj_sizes(img=,\n",
    "                                mask=,\n",
    "                                num_objects=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-fishing",
   "metadata": {
    "id": "endless-fishing"
   },
   "source": [
    "Well, apparently our filtered_mask isn't in one piece and is actually made up of many objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-latest",
   "metadata": {
    "id": "mathematical-latest"
   },
   "source": [
    "## Reducing Image Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-basketball",
   "metadata": {
    "id": "worst-basketball"
   },
   "source": [
    "We can connect some of the objects together using *pcv.fill_holes()*. This function fills all of the holes and connects several objects together with white pixels. You will use this function if you notice that there are \"holes\" in your object of interest (you will know that there are holes in the object if you see pixel size values on your object of interest).\n",
    "\n",
    "Inputs:\n",
    "* bin_img      = Filtered binary image data\n",
    "\n",
    "Returns:\n",
    "* filtered_img = image with objects filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-audit",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "a894f1978e0841129e6644b3d212bee3",
      "ddf7a69873ae445085df5a3bfba23cfb"
     ]
    },
    "id": "hourly-audit",
    "outputId": "0b61e5ee-71ef-406c-9611-15b4e1126e3b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "filled_mask = pcv.fill_holes(bin_img=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-garden",
   "metadata": {
    "id": "raised-garden"
   },
   "source": [
    "Look at the image with mask overlay we produced, does it contain the aspects of the plant that we care about (i.e., as much \"plant material\" as possible)? If so, good! If not, then we need to become more discrete with our binary mask or we can filter the salt/noise so we get as much plant material as we can within our contoured layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-dream",
   "metadata": {
    "id": "honest-dream"
   },
   "source": [
    "Let's see how our flood fill worked using *pcv.visualize.obj_sizes()* again. We may not be able to completely connect all of the pixels, but we can try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-oracle",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e66abd77ae5a46b2bb0e9b2e8f7b1237",
      "acf49285c01748f891ee5abcfb85078b"
     ]
    },
    "id": "european-oracle",
    "outputId": "4d093c0f-5be4-4263-c72e-8078b1e42a4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sizes = pcv.visualize.obj_sizes(img=,\n",
    "                                mask=,\n",
    "                                num_objects=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-relations",
   "metadata": {
    "id": "cooked-relations"
   },
   "source": [
    "Looks as though there are still 47 more objects in this image, likely background.\n",
    "\n",
    "## Fill\n",
    "We can use the Fill function to fill objects below a certain pixel size that are found in the background of your image. This will help us create a single object for analysis by minimizing the external noise.\n",
    "\n",
    "Inputs:\n",
    "* bin_img      = Binary image data (use the\n",
    "* size         = minimum object area size in pixels (integer)\n",
    "\n",
    "\n",
    "Returns:\n",
    "* filtered_img = image with objects filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-union",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b9c7970e12af4bab80e9191bc9b63a3d",
      "e2c1fc88292144f88e23621bc8cc6f29"
     ]
    },
    "id": "important-union",
    "outputId": "9aa563af-6112-4923-e1bc-7ba534a46111",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_mask = pcv.fill(bin_img=,\n",
    "                      size=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-equivalent",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "14bdbdeaf23944268333c2217bec83fd",
      "d8106d9d4ad94d5389cdbb505a02f6d0"
     ]
    },
    "id": "destroyed-equivalent",
    "outputId": "41152a82-22df-440f-cee8-0fc3a4712b8f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sizes = pcv.visualize.obj_sizes(img=,\n",
    "                                mask=,\n",
    "                                num_objects=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-fisher",
   "metadata": {
    "id": "backed-fisher"
   },
   "source": [
    "# Gathering Information From Your Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-sweden",
   "metadata": {
    "id": "matched-sweden"
   },
   "source": [
    "We're almost there! Now we just need to analyze our plant and see what information we can pull out!\n",
    "\n",
    "## Extract size data from your sample using *pcv.analyze.size()*\n",
    "Inputs:\n",
    "* img          = RGB image data for plotting\n",
    "* labeled_mask = Labeled mask of objects (32-bit).\n",
    "* n_labels     = Total number expected individual objects (default = 1).\n",
    "* label        = Optional label parameter, modifies the variable name of observations recorded (default = \"default\").\n",
    "\n",
    "Returns:\n",
    "* analysis_image = Diagnostic image showing measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-atlas",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "0a4069122a524b46a8c30fc29936b51f",
      "5ec431f7d4a44611b66705455a852db0"
     ]
    },
    "id": "owned-atlas",
    "outputId": "7db40720-4ce5-4187-9674-42ebf8a207dd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Measure the shape of the plant\n",
    "shape_img = pcv.analyze.size(img=,\n",
    "                             labeled_mask=,\n",
    "                             label=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-producer",
   "metadata": {
    "id": "subsequent-producer"
   },
   "source": [
    "## Extract color traits from your sample using *pcv.analyze.color()*\n",
    "\n",
    "Inputs:\n",
    "\n",
    "* img           = RGB image for debugging\n",
    "* labeled_mask  = Grayscale mask with unique pixel value per object of interest\n",
    "* n_labels      = Total number expected individual objects (default = 1).\n",
    "* colorspaces   = 'all', 'rgb', 'lab', or 'hsv' (default = 'hsv')\n",
    "* label         = Modifies the variable name of observations recorded (default = \"default\").\n",
    "\n",
    "Returns:\n",
    "* analysis_image  = histogram output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-mexican",
   "metadata": {
    "id": "greek-mexican",
    "outputId": "ae8affcc-4474-4b1e-c941-d18d86774bf2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Measure the color properties of the plant\n",
    "color_hist = pcv.analyze.color(rgb_img=,\n",
    "                               labeled_mask=,\n",
    "                               colorspaces=\"\",\n",
    "                               label=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-picnic",
   "metadata": {
    "id": "comparable-picnic",
    "tags": []
   },
   "source": [
    "## Saving results\n",
    "Analyzing a single plant is just the inital step of developing a workflow capable of performing high-throughput phenotyping. You will test your workflow on an increasing subset of image data to ensure its accuracy.\n",
    "* *A recommended subsetted dataset schedule for training your algorithm is 1 image > 5 images > 20 images > 50 images*.\n",
    "* Once you have developed sufficient accuracy with your workflow, then you will be ready to prepare a script for parallelization of the entirety of your image dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-letters",
   "metadata": {
    "id": "direct-letters"
   },
   "outputs": [],
   "source": [
    "# We will collect the data stored from *pcv.analyze.size()* and *pcv.analyze_color()* and save it as a Comma-Separated Values (CSV) files.\n",
    "# The filename will be set to *plant1_result*.\n",
    "pcv.outputs.save_results(,\n",
    "                         outformat=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-interview",
   "metadata": {
    "id": "exempt-interview"
   },
   "source": [
    "You can download the CSV file and view the attributes saved."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
